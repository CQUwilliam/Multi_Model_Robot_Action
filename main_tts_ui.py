# main.py
# åŸé”™è¯¯å¯¼å…¥æ–¹å¼
from pyaudio import PyAudio, paInt16
import tkinter as tk
from tkinter import ttk
import threading
import time
# ä¿®æ­£ä¸º
import pyaudio
import wave
import speech_recognition as sr
# å¯¼å…¥æ‰€éœ€çš„åº“
import openai
import requests
import cv2
import pyrealsense2 as rs
import numpy as np
import base64
import custom_urx as urx
import json
from PIL import Image
from scipy.spatial.transform import Rotation as R
from custom_urx.robotiq_two_finger_gripper import Robotiq_Two_Finger_Gripper

global text_record

# è®¾ç½®OpenAIçš„APIå¯†é’¥
openai.api_key = 'YOUR_OPENAI_API_KEY'  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„OpenAI APIå¯†é’¥
class RobotUI(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("æ™ºèƒ½æœºæ¢°è‡‚æ§åˆ¶ç³»ç»Ÿ")
        self.geometry("800x600")
        self.configure(bg="#2E3440")
        
        # å½•éŸ³æ§åˆ¶å˜é‡
        self.is_recording = False
        self.audio_frames = []
        
        # åˆå§‹åŒ–UIç»„ä»¶
        self.create_widgets()
        
    def create_widgets(self):
        style = ttk.Style()
        style.theme_use('clam')
        style.configure('TButton', font=('Arial', 12), padding=6)
        style.configure('TLabel', background='#2E3440', foreground='#D8DEE9')
        
        # ä¸»å®¹å™¨
        main_frame = ttk.Frame(self)
        main_frame.pack(expand=True, fill=tk.BOTH, padx=20, pady=20)
        
        # çŠ¶æ€æ˜¾ç¤ºåŒº
        self.status_label = ttk.Label(main_frame, text="å°±ç»ª", font=('Arial', 14))
        self.status_label.pack(pady=10)
        
        # æ³¢å½¢å¯è§†åŒ–ç”»å¸ƒ
        self.canvas = tk.Canvas(main_frame, bg="#3B4252", height=100)
        self.canvas.pack(fill=tk.X, pady=10)
        
        # æ§åˆ¶æŒ‰é’®åŒº
        btn_frame = ttk.Frame(main_frame)
        btn_frame.pack(pady=20)
        
        self.record_btn = ttk.Button(
            btn_frame, 
            text="ğŸ¤ å¼€å§‹å½•éŸ³", 
            command=self.toggle_recording,
            style='TButton'
        )
        self.record_btn.pack(side=tk.LEFT, padx=10)
        
        self.task_btn = ttk.Button(
            btn_frame,
            text="ğŸ¤– æ‰§è¡Œä»»åŠ¡",
            command=self.start_task,
            state=tk.DISABLED
        )
        self.task_btn.pack(side=tk.LEFT, padx=10)
        
        # è¯†åˆ«ç»“æœæ˜¾ç¤º
        self.result_text = tk.Text(
            main_frame, 
            height=8, 
            bg="#434C5E", 
            fg="#D8DEE9",
            font=('Arial', 12)
        )
        self.result_text.pack(fill=tk.BOTH, expand=True)
    
    def toggle_recording(self):
        """åˆ‡æ¢å½•éŸ³çŠ¶æ€"""
        if not self.is_recording:
            self.start_recording()
        else:
            self.stop_recording()
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        self.is_recording = True
        self.record_btn.config(text="â¹ åœæ­¢å½•éŸ³")
        self.status_label.config(text="å½•éŸ³ä¸­...")
        self.audio_frames = []
        
        # å¯åŠ¨å½•éŸ³çº¿ç¨‹
        self.audio_thread = threading.Thread(target=self.record_audio)
        self.audio_thread.start()
        
        # å¯åŠ¨æ³¢å½¢åŠ¨ç”»
        self.animate_wave()
    
    def stop_recording(self):
        global text_record
        """åœæ­¢å½•éŸ³"""
        self.is_recording = False
        self.record_btn.config(text="ğŸ¤ å¼€å§‹å½•éŸ³")
        self.status_label.config(text="å½•éŸ³ç»“æŸ")
        
        # ä¿å­˜å½•éŸ³æ–‡ä»¶
        self.save_recording()
        
        # è¯­éŸ³è¯†åˆ«
        text_record = speech_recognition()
        self.show_result(f"è¯†åˆ«ç»“æœï¼š{text_record}")
        self.task_btn.config(state=tk.NORMAL)

    
    def record_audio(self):
        """å½•éŸ³çº¿ç¨‹å‡½æ•°"""
        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )
        
        while self.is_recording:
            data = stream.read(1024)
            self.audio_frames.append(data)
        
        stream.stop_stream()
        stream.close()
        p.terminate()
    
    def save_recording(self):
        """ä¿å­˜å½•éŸ³æ–‡ä»¶"""
        with wave.open('temp.wav', 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(pyaudio.get_sample_size(pyaudio.paInt16))
            wf.setframerate(16000)
            wf.writeframes(b''.join(self.audio_frames))
    
    def animate_wave(self):
        """æ³¢å½¢åŠ¨ç”»"""
        if self.is_recording:
            self.canvas.delete("wave")
            width = self.canvas.winfo_width()
            height = self.canvas.winfo_height()
            
            for i in range(0, width, 10):
                amplitude = np.random.randint(10, 40)
                self.canvas.create_line(
                    i, height/2 - amplitude,
                    i+8, height/2 + amplitude,
                    fill="#88C0D0",
                    tags="wave",
                    width=2
                )
            self.after(100, self.animate_wave)
    
    def show_result(self, text):
        """æ˜¾ç¤ºè¯†åˆ«ç»“æœ"""
        self.result_text.delete(1.0, tk.END)
        self.result_text.insert(tk.END, text)
    
    def start_task(self):
        """å¯åŠ¨ä»»åŠ¡æ‰§è¡Œ"""
        self.task_btn.config(state=tk.DISABLED)
        task_thread = threading.Thread(target=self.run_robot_task)
        task_thread.start()
    
    def run_robot_task(self):
        """æœºæ¢°è‡‚ä»»åŠ¡çº¿ç¨‹"""
        # åœ¨è¿™é‡Œè°ƒç”¨åŸæœ‰çš„main()å‡½æ•°é€»è¾‘
        self.status_label.config(text="ä»»åŠ¡æ‰§è¡Œä¸­...")
        try:
            # è°ƒç”¨åŸæœ‰çš„mainå‡½æ•°é€»è¾‘
            main()
        except Exception as e:
            self.show_result(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™ï¼š{str(e)}")
        finally:
            self.status_label.config(text="ä»»åŠ¡å®Œæˆ")
def record(DURATION=5):
    """å½•éŸ³å‡½æ•°"""
    CHUNK = 1024
    FORMAT = paInt16
    CHANNELS = 1
    RATE = 16000
    
    p = PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    
    print("å¼€å§‹å½•éŸ³...")
    frames = []
    for _ in range(0, int(RATE / CHUNK * DURATION)):
        data = stream.read(CHUNK)
        frames.append(data)
    
    print("å½•éŸ³ç»“æŸ")
    stream.stop_stream()
    stream.close()
    p.terminate()
    
    # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
    with wave.open('temp.wav', 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

def speech_recognition():
    """è¯­éŸ³è¯†åˆ«å‡½æ•°"""
    r = sr.Recognizer()
    with sr.AudioFile('temp.wav') as source:
        audio = r.record(source)
    
    try:
        text = r.recognize_google(audio, language='zh-CN')
        print("è¯†åˆ«ç»“æœï¼š" + text)
        return text
    except sr.UnknownValueError:
        print("æ— æ³•è¯†åˆ«éŸ³é¢‘")
        return ""
    except sr.RequestError as e:
        print(f"æœåŠ¡é”™è¯¯ï¼š{e}")
        return ""

# å®šä¹‰GPT-4ä»»åŠ¡è§„åˆ’å‡½æ•°
def task_planning(task_description):
    """
    ä½¿ç”¨GPT-4å°†ä»»åŠ¡æè¿°è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ã€‚
    """
    prompt = f"""
å°†ä»¥ä¸‹ä»»åŠ¡åˆ†è§£ä¸ºæœºå™¨äººå¯æ‰§è¡Œçš„æ­¥éª¤ï¼Œå¹¶ä½¿ç”¨JSONæ ¼å¼è¾“å‡ºã€‚æ¯ä¸ªæ­¥éª¤åº”åŒ…æ‹¬ï¼š
- actionï¼šè¦æ‰§è¡Œçš„åŠ¨ä½œï¼ˆå¦‚detectã€graspã€placeï¼‰
- objectï¼šåŠ¨ä½œæ¶‰åŠçš„å¯¹è±¡
- purposeï¼šå¦‚æœæ˜¯æ£€æµ‹åŠ¨ä½œï¼Œè¯´æ˜æ£€æµ‹çš„ç›®çš„ï¼ˆå¦‚graspæˆ–placeï¼‰
- targetï¼šå¦‚æœæ˜¯æ”¾ç½®åŠ¨ä½œï¼ŒæŒ‡æ˜æ”¾ç½®çš„ç›®æ ‡

ä»»åŠ¡æè¿°ï¼š
â€œ{task_description}â€

è¯·è¾“å‡ºJSONæ ¼å¼çš„æ­¥éª¤åˆ—è¡¨ã€‚
"""
    response = openai.Completion.create(
        engine="gpt-4",  # å¦‚æœæœ‰GPT-4æƒé™ï¼Œå¯ä»¥ä½¿ç”¨"gpt-4"
        prompt=prompt,
        max_tokens=500,
        n=1,
        stop=None,
        temperature=0
    )
    steps = response.choices[0].text.strip()
    return steps


# ä»Realsenseç›¸æœºè·å–å›¾åƒ
def get_camera_images(pipeline,align):
    frames = pipeline.wait_for_frames()
    frames = align.process(frames)
    color_frame = frames.get_color_frame()
    depth_frame = frames.get_depth_frame()

    # å°†å›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
    color_image = np.asanyarray(color_frame.get_data())
    depth_image = np.asanyarray(depth_frame.get_data())
    cv2.imwrite('/home/cavalier/lgx/llmrobot/images/image.png',color_image)
    cv2.imwrite('/home/cavalier/lgx/llmrobot/images/depth.png',depth_image)

# åƒç´ åæ ‡å’Œæ·±åº¦å€¼è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»ä¸‹çš„3Dåæ ‡
def pixel_to_camera_coords(centroid, depth_value, intrinsics):
    x = (centroid[1] - intrinsics[0][2]) / intrinsics[0][0] * depth_value
    y = (centroid[0] - intrinsics[1][2]) / intrinsics[1][1] * depth_value
    z = depth_value
    return np.array([x, y, z,1]).reshape(4,1)

# åæ ‡è½¬æ¢
def transform_coordinates(camera_coords):
    # ä½¿ç”¨å·²çŸ¥çš„å¤–å‚çŸ©é˜µè¿›è¡Œè½¬æ¢
    transformation_matrix = np.eye(4)  # è¯·æ›¿æ¢ä¸ºå®é™…çš„å¤–å‚çŸ©é˜µ
    camera_coords_homogeneous = np.append(camera_coords, 1)
    base_coords = transformation_matrix.dot(camera_coords_homogeneous)
    return base_coords[:3]

# è½¬æ¢æŠ“å–å§¿æ€
def transform_grasp_pose(grasp_pose):
    position = np.array(grasp_pose['position'])
    rotation_matrix = np.array(grasp_pose['rotation'])
    # è½¬æ¢ä½ç½®
    base_position = transform_coordinates(position)
    # è½¬æ¢æ–¹å‘
    # éœ€è¦å°†æ—‹è½¬çŸ©é˜µä¹Ÿè½¬æ¢åˆ°æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
    base_rotation_matrix = rotation_matrix  # ç®€åŒ–å¤„ç†ï¼Œéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    return {'position': base_position, 'rotation': base_rotation_matrix}

def homogeneous_matrix(rotation, translation):
    """
    æ ¹æ®æ—‹è½¬çŸ©é˜µå’Œå¹³ç§»å‘é‡æ„é€ é½æ¬¡å˜æ¢çŸ©é˜µ
    """
    T = np.eye(4)
    T[:3, :3] = rotation
    T[:3, 3] = translation
    return T
# æ§åˆ¶UR5æœºæ¢°è‡‚æŠ“å–
def execute_grasp(robot,gripper,translation_cam_obj, rotation_cam_obj,width):
    print('\n')
    print('\n')
    print("======æ­£åœ¨æ‰§è¡ŒæŠ“å–======")
    
    T=np.array([[0,0,1],[0,1,0],[-1,0,0]])  # best
    # T=np.array([[0,1,0],[0,0,1],[1,0,0]])
    # T=np.array([[0,0,1],[0,1,0],[1,0,0]])

    # # è®¡ç®—å³æ‰‹ç³»æ—‹è½¬çŸ©é˜µ R_rh
    rotation_cam_obj = rotation_cam_obj @ T


    # ç›¸æœºåˆ°æœ«ç«¯çš„å˜æ¢çŸ©é˜µMï¼ˆ4x4ï¼‰
    M_cam2ee = np.load('/home/cavalier/lgx/llmrobot/config/hand_eye_transform.npy') # è¯·æ›¿æ¢ä¸ºå®é™…çš„æ•°å€¼

    # è®¡ç®—æœ«ç«¯åˆ°ç›¸æœºçš„å˜æ¢çŸ©é˜µï¼ˆM_cam2eeçš„é€†ï¼‰
    # T_ee_cam = np.linalg.inv(M_cam2ee)
    T_ee_cam=M_cam2ee

    # æ„é€ ç‰©ä½“åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„é½æ¬¡å˜æ¢çŸ©é˜µ
    T_cam_obj = homogeneous_matrix(rotation_cam_obj, translation_cam_obj)

    # è®¡ç®—ç‰©ä½“åœ¨æœ«ç«¯åæ ‡ç³»ä¸‹çš„ä½å§¿
    T_ee_obj = np.dot(T_ee_cam, T_cam_obj)



    # è·å–å½“å‰æœ«ç«¯åœ¨åŸºåº§åæ ‡ç³»ä¸‹çš„ä½å§¿çŸ©é˜µï¼ˆ4x4ï¼‰
    T_base_ee = robot.get_pose().get_matrix()

    # è®¡ç®—ç‰©ä½“åœ¨åŸºåº§åæ ‡ç³»ä¸‹çš„ä½å§¿
    T_base_obj = np.dot(T_base_ee, T_ee_obj)

    # ä»T_base_objä¸­æå–æ—‹è½¬çŸ©é˜µå’Œå¹³ç§»å‘é‡
    rotation_base_obj = T_base_obj[:3, :3]
    translation_base_obj = T_base_obj[:3, 3]

    # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºæ—‹è½¬å‘é‡ï¼ˆè½´è§’è¡¨ç¤ºï¼‰

    r = R.from_matrix(rotation_base_obj)
    rot_vec = r.as_rotvec()
    
    # tmp=translation_base_obj[0]
    # tmp1=translation_base_obj[0,0]
    # tmp3=translation_base_obj[0][0][0]
    # tmp2=float(translation_base_obj[0][0][0])
    # tmp3=tmp2.item()

    pose = [
    float(translation_base_obj[0,0]),
    float(translation_base_obj[1,0]),
    float(translation_base_obj[2,0])+0.01,
    2.709342051317487, 1.463335081708574, -0.08384753609461765
    # rot_vec[0],
    # rot_vec[1],
    # rot_vec[2]
    # 0
    ]

    # aa = [np.rad2deg(rot_vec[0]), np.rad2deg(rot_vec[1]),np.rad2deg(rot_vec[2])]
    # print(pose)

    # ç§»åŠ¨æœºå™¨äººåˆ°ç›®æ ‡ä½å§¿
    robot.movel(pose, acc=0.01, vel=0.08)
    
    # robot.movel(pose, acc=0.1, vel=0.05)
    # ç­‰å¤¹çˆªå¥½äº†å†å†™
    gripper.close_gripper()
    

# æ§åˆ¶UR5æœºæ¢°è‡‚æ”¾ç½®
def execute_place(robot,gripper, place_pose):
    print('\n')
    print('\n')
    print("======æ­£åœ¨æ‰§è¡Œæ”¾ç½®======")
    
    # approach_pose = place_pose + np.array([0, 0, 0.1,0,0,0])  # ä¸Šæ–¹10cm
    # robot.movel(approach_pose.tolist(), wait=True)
    robot.movel(place_pose, acc=0.01,vel=0.08)
    # æ§åˆ¶å¤¹çˆªæ‰“å¼€
    gripper.open_gripper()
    

# ä¸»ç¨‹åº
def main():
    # # -------------------------------tts-------------------------------------
    # # è¯­éŸ³/æŒ‡ä»¤è¾“å…¥æ¨¡å—
    # while True:
    #     start_record_ok = input('æ˜¯å¦å¼€å¯å½•éŸ³ï¼Ÿ\n'
    #                             'è¾“å…¥æ•°å­—å½•éŸ³æŒ‡å®šæ—¶é•¿ï¼ˆç§’ï¼‰\n'
    #                             'æŒ‰ k ä½¿ç”¨é”®ç›˜è¾“å…¥\n'
    #                             'æŒ‰ c ä½¿ç”¨é»˜è®¤æŒ‡ä»¤\n'
    #                             '>>> ')
        
    #     task_description = ""
    #     if str.isnumeric(start_record_ok):
    #         DURATION = int(start_record_ok)
    #         record(DURATION=DURATION)
    #         task_description = speech_recognition()
    #     elif start_record_ok.lower() == 'k':
    #         task_description = input("è¯·è¾“å…¥ä»»åŠ¡æŒ‡ä»¤ï¼š")
    #     elif start_record_ok.lower() == 'c':
    #         task_description = "å°†ç»¿è‰²è‹¹æœæ”¾åˆ°çº¢è‰²æ¯å­ä¸Š"
    #     else:
    #         print("æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°é€‰æ‹©")
    #         continue
        
    #     if task_description.strip():
    #         break
    # # -------------------------------tts-------------------------------------
    # print('\n')
    print('\n')
    print("======å¼€å§‹æ‰§è¡Œä»»åŠ¡======")
    hand_eye_transform = np.load('/home/cavalier/lgx/llmrobot/config/hand_eye_transform.npy')
    # åˆå§‹åŒ–UR5æœºæ¢°è‡‚
    robot = urx.Robot("192.168.1.40")  # è¯·æ›¿æ¢ä¸ºUR5çš„å®é™…IPåœ°å€
    gripper=Robotiq_Two_Finger_Gripper(robot)
    init_pose=[-0.07408505689348382, 0.48661989071612584, 0.23827063269992171, 2.264987695687536, 2.147798631721059, 0.10441536241257253]
    robot.movel(init_pose,acc=0.1,vel=0.08)
    gripper.open_gripper()
    pipeline = rs.pipeline()
    config = rs.config()
    # config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    # config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
    config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
    pipeline.start(config)

    # å¯¹é½æ·±åº¦å¸§åˆ°å½©è‰²å¸§
    align_to = rs.stream.color
    align = rs.align(align_to)
    # æ•è·ä¸€å¸§RGBå’Œæ·±åº¦å›¾åƒ
    for _ in range(30):  # è·³è¿‡å‰é¢çš„å‡ å¸§ï¼Œç¡®ä¿å›¾åƒç¨³å®š
        frames = pipeline.wait_for_frames()
        
    intrinsics = [[906.6353759765625, 0, 654.9713745117188], [0, 906.5818481445312, 358.79400634765625], [0, 0, 1]]

    # # è·å–ä»»åŠ¡æè¿°
    # task_description = "å°†çº¢è‰²æ–¹å—æ”¾åˆ°ç»¿è‰²æ–¹å—ä¸Š"
    #
    # # ä½¿ç”¨GPT-4è¿›è¡Œä»»åŠ¡è§„åˆ’
    # steps = task_planning(task_description)
    # print("ä»»åŠ¡è§„åˆ’ç»“æœï¼š", steps)
    
    ## try
    steps=[
  {
    "action": "detect",
    "object": "green apple",
    "purpose": "grasp"
  },
  {
    "action": "grasp",
    "object": "green apple"
  },
  {
    "action": "detect",
    "object": "red cup",
    "purpose": "place"
  },
  {
    "action": "place",
    "object": "green apple",
    "target": "red cup"
  }]
  

  
    steps_2=[
  {
    "action": "detect",
    "object": "red apple",
    "purpose": "grasp"
  },
  {
    "action": "grasp",
    "object": "red apple"
  },
  {
    "action": "detect",
    "object": "green cup",
    "purpose": "place"
  },
  {
    "action": "place",
    "object": "red apple",
    "target": "red cup"
  }
]


    # è§£ææ­¥éª¤
    # steps = json.loads(steps)

    # åˆå§‹åŒ–å˜é‡
    mask = None
    place_position = None

    # åœ¨æ­¤å¤„åˆ¤æ–­ä¸€ä¸‹ç”¨stepsè¿˜æ˜¯steps2(çº¢æ”¾ç»¿è¿˜æ˜¯ç»¿æ”¾çº¢)


    for step in steps:
        action = step['action']
        if action == 'detect':
            print('\n')
            print('\n')
            print("======æ­£åœ¨æ‰§è¡Œæ£€æµ‹ç¨‹åº======")
            
            robot.movel(init_pose,vel=0.08,acc=0.1)
            object_name = step['object']
            purpose = step.get('purpose', '')
            # è·å–ç›¸æœºå›¾åƒ
            get_camera_images(pipeline,align)
            # è°ƒç”¨SAMæœåŠ¡
            sam_payload = {
                'image_path': '/home/cavalier/lgx/llmrobot/images/image.png',
                'text_prompt': object_name
            }
            sam_response = requests.post('http://localhost:5000/segment', json=sam_payload)
            
            if sam_response.status_code == 200:
                mask_path='/home/cavalier/lgx/llmrobot/images/mask.png'
                if purpose == 'grasp':
                    pass
                elif purpose == 'place':
                    mask = np.array(Image.open(mask_path))
                    # è®¡ç®—æ”¾ç½®ä½ç½®
                    indices = np.argwhere(mask)
                    centroid = indices.mean(axis=0)
                    depth_image= np.array(Image.open('/home/cavalier/lgx/llmrobot/images/depth.png'))
                    depth_value = depth_image[int(centroid[0]), int(centroid[1])]/1000.0 # x0.001?
                    # å°†åƒç´ åæ ‡å’Œæ·±åº¦å€¼è½¬æ¢ä¸ºç›¸æœºåæ ‡ç³»ä¸‹çš„3Dåæ ‡
                    point_in_camera = pixel_to_camera_coords(centroid, depth_value, intrinsics)
                    # è½¬æ¢åˆ°æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
                    # print(object_name)
                    # è·å–å½“å‰æœºæ¢°è‡‚æœ«ç«¯ä½å§¿
                    # tcp_pose_list = list(robot.get_pos())  # è¿”å›[x, y, z, rx, ry, rz]
                    tcp_pose_list = robot.getl()
                    # æå–ä½ç½®å’Œæ–¹å‘
                    position = np.array(tcp_pose_list[:3])  # [x, y, z]
                    orientation = np.array(tcp_pose_list[3:])  # [rx, ry, rz]ï¼Œæ—‹è½¬å‘é‡
                    # orientation[:2] = 0

                    # å°†æ—‹è½¬å‘é‡è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
                    rotation_vector = orientation  # æ—‹è½¬å‘é‡
                    rotation = R.from_rotvec(rotation_vector)
                    rotation_matrix = rotation.as_matrix()  # 3x3æ—‹è½¬çŸ©é˜µ

                    # æ„å»º4x4é½æ¬¡å˜æ¢çŸ©é˜µ
                    # tcp_pose_matrix = np.eye(4)
                    # tcp_pose_matrix[:3, :3] = rotation_matrix
                    # tcp_pose_matrix[:3, 3] = position

                    T_base_tcp = np.eye(4)
                    T_base_tcp[:3, :3] = rotation_matrix
                    T_base_tcp[:3, 3] = position
                    # print(f'T_base_tcp{T_base_tcp}')

                    # è®¡ç®—ç›®æ ‡ç‚¹åœ¨æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»ä¸‹çš„åæ ‡
                    # T_base_tcp * T_tcp_cam * point_in_camera = point_in_base
                    point_in_base = T_base_tcp @ hand_eye_transform @ point_in_camera
                    
                    place_pose=robot.getl()
                    place_pose[:3]=point_in_base[:3].flatten()
                    # åœ¨ä¸Šé¢3cmæ”¾
                    place_pose[2]+=0.04
                    # print(f'æ”¾ç½®é«˜åº¦{place_pose}')
                    
                    # place_position = point_in_base[:3].flatten()
            else:
                print(f"SAMæœåŠ¡è°ƒç”¨å¤±è´¥ï¼š{sam_response.status_code}")
        elif action == 'grasp':
            # ç¼–ç RGBå›¾åƒã€æ·±åº¦å›¾åƒå’Œæ©ç 
            # è°ƒç”¨GraspNetæœåŠ¡
            graspnet_payload = {
                'color_image_path': '/home/cavalier/lgx/llmrobot/images/image.png',
                'depth_image_path': '/home/cavalier/lgx/llmrobot/images/depth.png',
                'mask_path':'/home/cavalier/lgx/llmrobot/images/mask.png'
            }
            graspnet_response = requests.post('http://localhost:6001/grasp', json=graspnet_payload)
            if graspnet_response.status_code == 200:
                translation=np.load('/home/cavalier/lgx/llmrobot/gripper_info/translation.npy')
                rotation=np.load('/home/cavalier/lgx/llmrobot/gripper_info/rotation.npy')
                width=np.load('/home/cavalier/lgx/llmrobot/gripper_info/width.npy')
                if translation is not None:
                    # æ‰§è¡ŒæŠ“å–
                    execute_grasp(robot,gripper,translation, rotation, width)
                else:
                    print("æœªèƒ½è·å–æŠ“å–å§¿æ€")
            else:
                print(f"GraspNetæœåŠ¡è°ƒç”¨å¤±è´¥ï¼š{graspnet_response.status_code}")

        elif action == 'place':
            if place_pose is not None:
                # æ‰§è¡Œæ”¾ç½®
                execute_place(robot,gripper, place_pose)
            else:
                print("æœªæ‰¾åˆ°æ”¾ç½®ä½ç½®ï¼Œæ— æ³•æ‰§è¡Œæ”¾ç½®")
        else:
            print(f"æœªçŸ¥çš„åŠ¨ä½œï¼š{action}")
    
    # time.sleep(30)
    # get_camera_images(pipeline,align)

    # åœæ­¢ç›¸æœº
    pipeline.stop()

    # å…³é—­æœºæ¢°è‡‚è¿æ¥
    robot.close()

if __name__ == "__main__":
    # main()
    ui = RobotUI()
    ui.mainloop()
